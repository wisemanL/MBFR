Environments.NS_Reacher
Module path:  Environments.NS_Reacher NS_Reacher
Dynamically loaded from:  <class 'Environments.NS_Reacher.NS_Reacher'>
Src.Algorithms.ProDyna
Module path:  Src.Algorithms.ProDyna ProDyna
Dynamically loaded from:  <class 'Src.Algorithms.ProDyna.ProDyna'>
=====Configurations=====
 Namespace(NN_basis_dim='32', Policy_basis_dim='32', actor_lr=0.005, algo_name='ProDyna', alpha_Q=0.99, base=0, batch_size=1000, buffer_size=1000, debug=True, delta=1, discrete_change=True, entropy_lambda=0.1, env_name='NS_Reacher', experiment='Test_runfolder', extrapolator_basis='Poly', folder_suffix='Default', fourier_coupled=True, fourier_k=7, fourier_order=-1, gamma=0.99, gauss_std=1.5, gpu=0, gradient_step=1, grid_size=20, howmanychange=7, hyper='default', importance_clip=10.0, inc=1, log_output='term_file', max_episodes=1500, max_episodes_realTrajectory=1000, max_episodes_syntheticTrajectory=10, max_inner=150, max_step_syntheticTrajectory=3, max_steps=500, optim='rmsprop', oracle=-1000, raw_basis=True, restore=False, reward_function_predict_lag=1, reward_function_reference_lag=10, sars_batchSize_for_policyUpdate=100, save_count=10, save_model=True, seed=2, speed=4, state_lr=0.001, summary=True, swarm=False, timestamp='11|27|15:40:10')
Actions space: 4 :: State space: 2
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([4, 2])), ('fc1.bias', torch.Size([4]))]
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([4, 2])), ('fc1.bias', torch.Size([4]))]
0 :: Rewards -6.000 :: steps: 13.00 :: Time: 0.011(0.00088/step) :: Entropy : 0.000 :: Grads : [[], []]
1 :: Rewards -12.000 :: steps: 13.00 :: Time: 0.254(0.01956/step) :: Entropy : 0.000 :: Grads : [[], []]
2 :: Rewards -18.000 :: steps: 13.00 :: Time: 0.052(0.00397/step) :: Entropy : 0.000 :: Grads : [[], []]
3 :: Rewards -24.000 :: steps: 13.00 :: Time: 0.051(0.00394/step) :: Entropy : 0.000 :: Grads : [[], []]
4 :: Rewards -30.000 :: steps: 13.00 :: Time: 0.052(0.00402/step) :: Entropy : 0.000 :: Grads : [[], []]
5 :: Rewards -36.000 :: steps: 13.00 :: Time: 0.105(0.00811/step) :: Entropy : 0.000 :: Grads : [[], []]
6 :: Rewards -42.000 :: steps: 13.00 :: Time: 0.049(0.00378/step) :: Entropy : 0.000 :: Grads : [[], []]
7 :: Rewards -48.000 :: steps: 13.00 :: Time: 0.048(0.00371/step) :: Entropy : 0.000 :: Grads : [[], []]
8 :: Rewards -54.000 :: steps: 13.00 :: Time: 0.051(0.00390/step) :: Entropy : 0.000 :: Grads : [[], []]
9 :: Rewards -60.000 :: steps: 13.00 :: Time: 1.304(0.10034/step) :: Entropy : 0.000 :: Grads : [[], []]
1
10 :: Rewards -66.000 :: steps: 13.00 :: Time: 1.586(0.12203/step) :: Entropy : 0.000 :: Grads : [[0.047742706, 0.11217053], []]
1
11 :: Rewards -72.000 :: steps: 13.00 :: Time: 1.587(0.12211/step) :: Entropy : 0.000 :: Grads : [[0.04071261, 0.091647096], []]
1
12 :: Rewards -78.000 :: steps: 13.00 :: Time: 1.594(0.12265/step) :: Entropy : 0.000 :: Grads : [[0.029851312, 0.06708245], []]
1
13 :: Rewards -84.000 :: steps: 13.00 :: Time: 1.582(0.12173/step) :: Entropy : 0.000 :: Grads : [[0.024106532, 0.056471918], []]
1
14 :: Rewards -90.000 :: steps: 13.00 :: Time: 1.621(0.12471/step) :: Entropy : 0.000 :: Grads : [[0.019304581, 0.042797405], []]
1
15 :: Rewards -96.000 :: steps: 13.00 :: Time: 1.594(0.12265/step) :: Entropy : 0.000 :: Grads : [[0.013731929, 0.029904407], []]
1
16 :: Rewards -102.000 :: steps: 13.00 :: Time: 1.592(0.12250/step) :: Entropy : 0.000 :: Grads : [[0.01189024, 0.024990132], []]
1
17 :: Rewards -108.000 :: steps: 13.00 :: Time: 1.648(0.12676/step) :: Entropy : 0.000 :: Grads : [[0.012270175, 0.027942296], []]
1
18 :: Rewards -114.000 :: steps: 13.00 :: Time: 1.609(0.12378/step) :: Entropy : 0.000 :: Grads : [[0.009895386, 0.02270291], []]
1
19 :: Rewards -120.000 :: steps: 13.00 :: Time: 1.614(0.12418/step) :: Entropy : 0.000 :: Grads : [[0.008303078, 0.018388268], []]
1
20 :: Rewards -126.000 :: steps: 13.00 :: Time: 1.587(0.12207/step) :: Entropy : 0.000 :: Grads : [[0.0072319615, 0.016478768], []]
1
21 :: Rewards -132.000 :: steps: 13.00 :: Time: 1.606(0.12355/step) :: Entropy : 0.000 :: Grads : [[0.0051644538, 0.012895058], []]
1
22 :: Rewards -138.000 :: steps: 13.00 :: Time: 1.573(0.12100/step) :: Entropy : 0.000 :: Grads : [[0.0048531317, 0.011515234], []]
1
23 :: Rewards -144.000 :: steps: 13.00 :: Time: 1.574(0.12105/step) :: Entropy : 0.000 :: Grads : [[0.005648772, 0.0127516175], []]
1
24 :: Rewards -150.000 :: steps: 13.00 :: Time: 1.573(0.12096/step) :: Entropy : 0.000 :: Grads : [[0.0057128845, 0.013171056], []]
