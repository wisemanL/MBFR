Environments.NS_Reacher
Module path:  Environments.NS_Reacher NS_Reacher
Dynamically loaded from:  <class 'Environments.NS_Reacher.NS_Reacher'>
Src.Algorithms.ProDyna
Module path:  Src.Algorithms.ProDyna ProDyna
Dynamically loaded from:  <class 'Src.Algorithms.ProDyna.ProDyna'>
=====Configurations=====
 Namespace(NN_basis_dim='32', Policy_basis_dim='32', actor_lr=0.005, algo_name='ProDyna', alpha_Q=0.99, base=0, batch_size=1000, buffer_size=1000, debug=True, delta=1, discrete_change=True, entropy_lambda=0.1, env_name='NS_Reacher', experiment='Test_runfolder', extrapolator_basis='Poly', folder_suffix='Default', fourier_coupled=True, fourier_k=7, fourier_order=-1, gamma=0.99, gauss_std=1.5, gpu=0, gradient_step=100, grid_size=10, howmanychange=0, hyper='default', importance_clip=10.0, inc=1, log_output='term_file', max_episodes=5000, max_episodes_syntheticTrajectory=10, max_inner=150, max_step_syntheticTrajectory=3, max_steps=500, optim='rmsprop', oracle=-1000, raw_basis=True, restore=False, reward_function_predict_lag=1, reward_function_reference_lag=10, sars_batchSize_for_policyUpdate=100, save_count=10, save_model=True, seed=2, speed=0, state_lr=0.001, summary=True, swarm=False, timestamp='12|1|21:4:22')
Actions space: 4 :: State space: 2
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([4, 2])), ('fc1.bias', torch.Size([4]))]
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([4, 2])), ('fc1.bias', torch.Size([4]))]
0 :: Rewards -6.000 :: steps: 13.00 :: Time: 0.009(0.00067/step) :: Entropy : 0.000 :: Grads : [[], []]
1 :: Rewards -12.000 :: steps: 13.00 :: Time: 0.372(0.02862/step) :: Entropy : 0.000 :: Grads : [[], []]
2 :: Rewards -18.000 :: steps: 13.00 :: Time: 0.099(0.00762/step) :: Entropy : 0.000 :: Grads : [[], []]
3 :: Rewards -24.000 :: steps: 13.00 :: Time: 0.097(0.00745/step) :: Entropy : 0.000 :: Grads : [[], []]
4 :: Rewards -30.000 :: steps: 13.00 :: Time: 0.095(0.00728/step) :: Entropy : 0.000 :: Grads : [[], []]
5 :: Rewards -36.000 :: steps: 13.00 :: Time: 0.095(0.00734/step) :: Entropy : 0.000 :: Grads : [[], []]
6 :: Rewards -42.000 :: steps: 13.00 :: Time: 0.095(0.00732/step) :: Entropy : 0.000 :: Grads : [[], []]
7 :: Rewards -48.000 :: steps: 13.00 :: Time: 0.105(0.00810/step) :: Entropy : 0.000 :: Grads : [[], []]
8 :: Rewards -54.000 :: steps: 13.00 :: Time: 0.097(0.00750/step) :: Entropy : 0.000 :: Grads : [[], []]
9 :: Rewards -60.000 :: steps: 13.00 :: Time: 0.422(0.03243/step) :: Entropy : 0.000 :: Grads : [[], []]
10 :: Rewards -66.000 :: steps: 13.00 :: Time: 1.075(0.08268/step) :: Entropy : 0.000 :: Grads : [[0.002649353, 0.005779897], []]
11 :: Rewards -72.000 :: steps: 13.00 :: Time: 1.028(0.07905/step) :: Entropy : 0.000 :: Grads : [[0.00086863013, 0.0032281936], []]
12 :: Rewards -78.000 :: steps: 13.00 :: Time: 1.001(0.07696/step) :: Entropy : 0.000 :: Grads : [[0.0035885712, 0.008658078], []]
